History of sequence-to-sequence models
Sequence-to-sequence models were introduced as an extension of traditional feedforward neural networks. Researchers realized the need for models that could handle variable-length input and output sequences, such as machine translation. The pioneering work of Sutskever et al. (2014) introduced the use of RNNs for seq2seq models.

Here are some main objectives of seq2seq models:

Translation: Translating a sequence from one domain to another (e.g., English to French).
Question answering: Generating a natural language response given an input sentence (e.g., chatbots).
Summarization: Summarizing a long document into a shorter sequence of sentences. And many more applications that involve sequence generation.
